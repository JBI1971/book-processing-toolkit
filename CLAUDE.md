# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**üìñ For organizational standards and coding conventions, see [docs/BEST_PRACTICES.md](docs/BEST_PRACTICES.md)**

## Project Overview

**Book Processing Toolkit** - Complete pipeline for processing books from raw JSON through cleaning, AI structuring, translation, footnotes, to final EPUB generation.

**Vision**: `Raw JSON ‚Üí Clean ‚Üí Structure ‚Üí Translate ‚Üí Footnotes ‚Üí EPUB`

**Current Status**:
- ‚úÖ JSON Cleaner (implemented)
- ‚úÖ Post-Processing Tools (chapter alignment, TOC restructuring)
- ‚úÖ Validation Tools (sanity checker, sequence validator, TOC alignment validator)
- ‚úÖ Content Structurer (implemented)
- ‚úÖ Batch Pipeline (6-stage processing with metadata enrichment)
- üöß Translator (placeholder)
- üöß Footnote Generator (placeholder)
- üöß EPUB Builder (placeholder)

## Development Commands

### Environment Setup
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
pip install -r requirements-dev.txt  # For development
```

### Running Processors

```bash
# JSON cleaning (no API needed)
python cli/clean.py --input book.json --output cleaned.json --language zh-Hant

# AI-powered structuring (requires OPENAI_API_KEY)
python cli/structure.py --input cleaned.json --output structured.json --max-workers 3

# Structure validation (requires OPENAI_API_KEY for AI classification)
python cli/validate_structure.py --input cleaned.json --output validation_report.json

# Future processors (placeholders)
python cli/translate.py --input structured.json --output translated.json --target-lang en
python cli/footnotes.py --input translated.json --output with_footnotes.json --style chicago
python cli/build_epub.py --input with_footnotes.json --output book.epub
```

### Testing
```bash
pytest                          # Run all tests
pytest tests/test_specific.py   # Run specific test
pytest -v                       # Verbose output
pytest --cov                    # Coverage report
```

### Code Quality
```bash
black processors ai utils cli   # Format code
flake8 processors ai utils cli  # Lint code
mypy processors ai utils cli    # Type checking
```

### Package Scripts (npm)
```bash
npm run clean      # Run JSON cleaner
npm run structure  # Run content structurer
npm run test       # Run pytest
npm run lint       # Run flake8
npm run format     # Run black
```

## Architecture

### Pipeline Structure

```
processors/
‚îú‚îÄ‚îÄ json_cleaner.py          [IMPLEMENTED] Raw JSON ‚Üí Discrete blocks
‚îú‚îÄ‚îÄ content_structurer.py    [IMPLEMENTED] Blocks ‚Üí Semantic types
‚îú‚îÄ‚îÄ structure_validator.py   [IMPLEMENTED] AI-powered TOC/structure validation
‚îú‚îÄ‚îÄ translator.py            [PLACEHOLDER] Translate content
‚îú‚îÄ‚îÄ footnote_generator.py    [PLACEHOLDER] Add scholarly notes
‚îî‚îÄ‚îÄ epub_builder.py          [PLACEHOLDER] Generate EPUB file

utils/
‚îú‚îÄ‚îÄ topology_analyzer.py         [IMPLEMENTED] Structure analysis without modification
‚îú‚îÄ‚îÄ sanity_checker.py            [IMPLEMENTED] Early validation with metadata enrichment
‚îú‚îÄ‚îÄ catalog_metadata.py          [IMPLEMENTED] Extract metadata from SQLite catalog
‚îú‚îÄ‚îÄ chapter_sequence_validator.py [IMPLEMENTED] Chinese chapter numbering validation
‚îú‚îÄ‚îÄ toc_alignment_validator.py   [IMPLEMENTED] OpenAI-powered TOC validation
‚îú‚îÄ‚îÄ fix_chapter_alignment.py     [IMPLEMENTED] Fix EPUB metadata mismatches
‚îú‚îÄ‚îÄ restructure_toc.py           [IMPLEMENTED] Convert TOC to structured navigation
‚îú‚îÄ‚îÄ clients/                     [IMPLEMENTED] API wrappers (OpenAI, Anthropic)
‚îî‚îÄ‚îÄ http/                        [IMPLEMENTED] HTTP with retry logic

scripts/
‚îî‚îÄ‚îÄ batch_process_books.py   [IMPLEMENTED] 6-stage pipeline with metadata enrichment

ai/
‚îî‚îÄ‚îÄ assistant_manager.py     [IMPLEMENTED] Manage OpenAI assistants

cli/
‚îú‚îÄ‚îÄ clean.py                 [IMPLEMENTED] CLI for json_cleaner
‚îú‚îÄ‚îÄ structure.py             [IMPLEMENTED] CLI for content_structurer
‚îú‚îÄ‚îÄ validate_structure.py    [IMPLEMENTED] CLI for structure_validator
‚îú‚îÄ‚îÄ translate.py             [PLACEHOLDER] CLI for translator
‚îú‚îÄ‚îÄ footnotes.py             [PLACEHOLDER] CLI for footnote_generator
‚îî‚îÄ‚îÄ build_epub.py            [PLACEHOLDER] CLI for epub_builder
```

### Data Flow

**Standard Batch Pipeline (6 Stages)**:
```
RAW INPUT (source JSON in individual directories)
   ‚Üì
[Stage 1] utils/topology_analyzer.py
   ‚Üí Analyze JSON structure without modifications
   ‚Üí Estimate tokens for AI processing
   ‚Üí Identify content locations and nesting depth
   ‚Üí Deterministic, no API calls
   ‚Üì
[Stage 2] utils/sanity_checker.py
   ‚Üí Extract metadata from catalog SQLite database
     (work_number, title, author, volume)
   ‚Üí Validate Chinese chapter numbering sequences
   ‚Üí Detect missing, duplicate, or out-of-order chapters
   ‚Üí Parse Chinese numerals: ‰∏Ä‰∫å‰∏â...ÂçÅÂªøÂçÖÂçå...ÁôæÂçÉ
   ‚Üí Non-fatal validation (continues on errors)
   ‚Üì
[Stage 3] processors/json_cleaner.py
   ‚Üí Recursive extraction from nested structures
   ‚Üí TOC auto-detection
   ‚Üí Block ID generation (block_0000, block_0001...)
   ‚Üí EPUB ID generation (heading_0, para_1...)
   ‚Üí Enrich with catalog metadata (work_number, title, author, volume)
   ‚Üì
CLEANED JSON (discrete blocks + metadata)
   ‚Üì
[Stage 4] utils/fix_chapter_alignment.py
   ‚Üí Fix EPUB metadata mismatches
   ‚Üí Match chapter titles to actual content headings
   ‚Üí Support Á¨¨NÂõû (hui) and Á¨¨NÁ´† (zhang) formats
   ‚Üí Support special numerals: Âªø (20), ÂçÖ (30), Âçå (40)
   ‚Üí Split combined chapters
   ‚Üí Deterministic, pattern-based
   ‚Üì
[Stage 5] utils/restructure_toc.py
   ‚Üí Convert TOC from text blob to structured list
   ‚Üí Create chapter references for EPUB navigation
   ‚Üí Handle blob format (space-separated entries)
   ‚Üí Generate TOC from chapters when missing
   ‚Üí Support Chinese numerals including Âªø/ÂçÖ/Âçå
   ‚Üí Fuzzy matching for variants
   ‚Üì
[Stage 6] Comprehensive Validation (3 validators)
   ‚Üí batch_process_books.py orchestrates multiple validators:

   A. processors/structure_validator.py
      - AI-powered chapter classification (front_matter, body, back_matter)
      - Detects special sections (preface, afterword, appendix)
      - Quality scoring

   B. utils/toc_chapter_validator.py
      - Extracts actual headings from content_blocks
      - OpenAI-powered semantic TOC/chapter title matching
      - Detects missing chapters, title mismatches
      - Handles Chinese numeral variations (Âªø/ÂçÖ/Âçå)

   C. utils/toc_body_count_validator.py [NEW]
      - Validates TOC entry count matches body chapter count
      - Identifies specific chapters missing from TOC
      - Detects extra TOC entries not in body
      - Fast, deterministic (no API calls)

   ‚Üí All validators run in parallel
   ‚Üí Results merged with error/warning categorization
   ‚Üí Requires OPENAI_API_KEY for validators A & B
   ‚Üì
VALIDATED JSON (ready for downstream processing)
   ‚Üì
processors/content_structurer.py [OPTIONAL]
   ‚Üí Chunk text if >4000 chars (200-char overlap)
   ‚Üí OpenAI Assistant API via threads
   ‚Üí Semantic classification (narrative, dialogue, verse...)
   ‚Üí Schema validation
   ‚Üí Retry logic (3 attempts, 2s delay, 300s timeout)
   ‚Üì
processors/translator.py [TODO]
   ‚Üí AI-powered translation
   ‚Üí Preserve structure
   ‚Üì
processors/footnote_generator.py [TODO]
   ‚Üí Cultural/historical notes
   ‚Üí Pronunciation guides
   ‚Üì
processors/epub_builder.py [TODO]
   ‚Üí EPUB 3.0 generation
   ‚Üí Internal linking via block IDs
   ‚Üì
FINAL EPUB FILE
```

## Key Implementation Details

### JSON Cleaner (processors/json_cleaner.py)

**Input Formats Supported**:
- `{chapters: [{title, content}]}` - Standard format
- `{sections: [{title, content}]}` - Alternative field names
- Content as string, HTML-like objects, or nested structures

**Block Extraction**:
- Recursive `extract_blocks_from_nodes()` handles arbitrary nesting
- Recognizes tags: h1-h6, p, div, section, article, body, ul, ol, li
- Generates sequential IDs: `block_0000`, `block_0001`...
- Creates EPUB IDs: `heading_0`, `para_1`, `text_2`, `list_3`

**TOC Detection**:
- Title keywords: "ÁõÆÈåÑ", "ÁõÆÂΩï", "contents", "table of contents", "toc"
- Structure heuristic (first chapter only): ‚â•5 lines with 70%+ having ‚â§15 chars

**Output Structure**:
```json
{
  "meta": {
    "title": "Book Title",
    "author": "Author Name",
    "work_number": "I0929",
    "volume": "a",
    "language": "zh-Hant",
    "schema_version": "2.0.0"
  },
  "structure": {
    "front_matter": {"toc": [...]},
    "body": {"chapters": [{"id", "title", "ordinal", "content_blocks": [...]}]},
    "back_matter": {}
  }
}
```

### Catalog Metadata Extractor (utils/catalog_metadata.py)

**Purpose**: Extract metadata from SQLite catalog database for enrichment

**Database Schema**:
- `works` table: work_id, work_number, title_chinese, title_english, author_chinese, author_english
- `work_files` table: work_id, directory_name, volume

**Classes**:
- `WorkMetadata` - Dataclass holding extracted metadata
- `CatalogMetadataExtractor` - Main extractor with query methods

**Key Methods**:
```python
def get_metadata_by_directory(self, directory_name: str) -> Optional[WorkMetadata]:
    """
    Extract metadata by directory name (e.g., 'wuxia_0117').
    Returns WorkMetadata with work_number, title, author, volume.
    """
```

**Usage**:
```python
extractor = CatalogMetadataExtractor('wuxia_catalog.db')
metadata = extractor.get_metadata_by_directory('wuxia_0117')
# Returns: WorkMetadata(work_number='I0929', title_chinese='ÁæÖÂâéÂ§´‰∫∫',
#                       author_chinese='Êú±Ë≤ûÊú®', volume=None)
```

### Chinese Chapter Sequence Validator (utils/chapter_sequence_validator.py)

**Purpose**: Validate Chinese chapter numbering sequences and detect gaps/duplicates

**Chinese Numeral Support**:
- Basic: ‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅ
- Special: Âªø (20), ÂçÖ (30), Âçå (40)
- Large: Áôæ (100), ÂçÉ (1000)

**Parsing Logic**:
```python
def parse_chinese_number(self, text: str) -> Optional[int]:
    """
    Parse Chinese numerals including special cases:
    - Âªø‰∏Ä ‚Üí 21 (20 + 1)
    - ÂçÖ‰∫î ‚Üí 35 (30 + 5)
    - Á¨¨‰∏âÂçÅ‰∫åÁ´† ‚Üí 32
    """
```

**Classes**:
- `SequenceIssue` - Dataclass for validation issues (gap, duplicate, out_of_order)
- `ChineseChapterSequenceValidator` - Main validator

**Common Issue Types**:
- `gap` - Missing chapter numbers (e.g., ch 1, 2, 4 missing 3)
- `duplicate` - Same chapter number appears twice
- `out_of_order` - Chapters not in ascending order
- `nonstandard_start` - Book starts at ch 2+ instead of ch 1

### Sanity Checker (utils/sanity_checker.py)

**Purpose**: Combined early validation with metadata enrichment (Stage 2)

**Integration**:
- Combines `CatalogMetadataExtractor` + `ChineseChapterSequenceValidator`
- Runs after topology analysis, before cleaning
- Non-fatal: continues processing even on errors

**Classes**:
- `SanityCheckResult` - Dataclass with metadata, issues, summary
- `BookSanityChecker` - Main checker

**Workflow**:
```python
checker = BookSanityChecker(catalog_path='wuxia_catalog.db')
result = checker.check(
    json_file=Path('book.json'),
    directory_name='wuxia_0117',
    strict_sequence=False  # Don't fail on sequence issues
)
# Returns: SanityCheckResult with metadata + sequence_issues
```

**Output**:
- `metadata`: WorkMetadata from catalog
- `sequence_issues`: List of SequenceIssue objects
- `has_errors`: Boolean indicating critical issues
- `summary`: Human-readable summary string

### Chapter Alignment Fixer (utils/fix_chapter_alignment.py)

**Purpose**: Fix EPUB metadata mismatches (Stage 4)

**What It Fixes**:
- Matches chapter titles to actual content headings
- Splits combined chapters (multiple headings in one chapter)
- Handles duplicate headings with ordinals

**Supported Formats**:
- Á¨¨NÂõû - Traditional episode format (hui)
- Á¨¨NÁ´† - Modern chapter format (zhang)
- Special numerals: Âªø (20), ÂçÖ (30), Âçå (40)

**Known Issue**:
‚ö†Ô∏è Assumes books start at Chapter 1. Some books (e.g., volume 3 of series) start at Chapter 2+, which causes incorrect chapter_number fields in TOC.

**Usage**:
```python
fixer = ChapterAlignmentFixer()
result = fixer.fix_chapter_alignment_in_file('cleaned_book.json')
# Modifies file in-place, returns fix count
```

### TOC Restructurer (utils/restructure_toc.py)

**Purpose**: Convert TOC from text blob to structured navigation (Stage 5)

**What It Does**:
- Parses space-separated TOC entries from blob format
- Creates chapter references (chapter_id, chapter_number)
- Generates TOC from chapters when missing
- Fuzzy matching for minor character variants (ËñÑ/Ê≥ä, Âà∞/Ëá≥)

**Structured TOC Format**:
```json
{
  "toc": [
    {
      "full_title": "Á¨¨‰∏ÄÁ´†„ÄÄÊ®ôÈ°å",
      "chapter_title": "Ê®ôÈ°å",
      "chapter_number": 1,
      "chapter_id": "chapter_0001"
    }
  ]
}
```

**Chinese Numeral Support**:
- Full character set including Âªø/ÂçÖ/Âçå
- Regex patterns updated in 2 locations (lines ~175 and ~362)

### TOC/Body Count Validator (utils/toc_body_count_validator.py)

**Purpose**: Validate that TOC entries match body chapters by count and chapter numbers

**What It Detects**:
- Missing chapters from TOC (chapters in body but not in TOC)
- Extra TOC entries (TOC references non-existent chapters)
- Count mismatches between TOC and body

**Classes**:
- `CountMismatchIssue` - Represents a chapter missing from or extra in TOC
- `CountValidationResult` - Complete validation result with issues
- `TOCBodyCountValidator` - Main validator

**Key Methods**:
```python
def validate_toc_body_alignment(self, cleaned_json: Dict[str, Any]) -> Dict[str, Any]:
    """
    Simplified validation that returns:
    - valid: bool
    - toc_count: int
    - body_count: int
    - missing_from_toc: list of chapter numbers in body but not TOC
    - extra_in_toc: list of chapter numbers in TOC but not body
    - missing_chapters: list of dicts with {chapter_num, title, id}
    """
```

**Usage**:
```python
from utils.toc_body_count_validator import validate_toc_body_alignment

result = validate_toc_body_alignment(cleaned_json)
if not result['valid']:
    for ch in result['missing_chapters']:
        print(f"Missing: Ch {ch['chapter_num']} - {ch['title']}")
```

**CLI Usage**:
```bash
# Full validation with detailed report
python utils/toc_body_count_validator.py input.json

# Simplified alignment check
python utils/toc_body_count_validator.py input.json --use-alignment

# Save report
python utils/toc_body_count_validator.py input.json --save-report
```

**Integration**: Automatically runs as part of Stage 6 validation in batch processing pipeline.

### TOC Alignment Validator (utils/toc_alignment_validator.py)

**Purpose**: OpenAI-powered semantic TOC/chapter title validation (basic, legacy)

**Validation Method**:
- Batch processing (20 TOC/chapter pairs per API call)
- GPT-4o-mini with low temperature (0.1) for consistency
- JSON response format for structured results

**Classes**:
- `AlignmentIssue` - Dataclass for mismatch issues
- `AlignmentResult` - Complete validation result
- `TOCAlignmentValidator` - Main validator

**Issue Types**:
- `mismatch` - TOC doesn't match chapter title
- `number_mismatch` - Chapter numbers don't align
- `missing_chapter` - TOC references non-existent chapter
- `typo` - Minor transcription error

**API Call Pattern**:
```python
validator = TOCAlignmentValidator(model='gpt-4o-mini', temperature=0.1)
result = validator.validate(cleaned_json)
# Returns: AlignmentResult with issues, confidence_score, summary
```

**Output Metrics**:
- `is_valid` - Boolean (True if no errors)
- `total_pairs` - Number of TOC/chapter pairs checked
- `matched_pairs` - Number of successful matches
- `confidence_score` - Percentage (0-100%)
- `issues` - List of AlignmentIssue objects with severity

### Comprehensive TOC/Chapter Validator (utils/toc_chapter_validator.py)

**Purpose**: Advanced validation that extracts actual chapter headings from content_blocks (Stage 6 - enhanced)

**Key Innovation**: Unlike the basic validator above, this validator extracts the **actual heading from content_blocks** to detect when:
- Book metadata is incorrectly treated as a chapter
- First chapter heading is buried in content
- TOC lists chapters that don't exist in the body
- Chapter topology doesn't match TOC structure

**Validation Workflow**:
1. Extract TOC entries from `front_matter.toc`
2. Extract **actual chapter headings** from each chapter's `content_blocks` (first heading-type block)
3. Parse chapter numbers from actual headings (Á¨¨NÁ´†/Âõû)
4. Compare TOC entries to actual chapter headings
5. Detect missing chapters, sequence gaps, duplicates
6. Use OpenAI for semantic validation of ambiguous mismatches

**Classes**:
- `TOCChapterValidator` - Main validator with heading extraction
- `ChapterHeading` - Extracted heading data (chapter_index, actual_heading, chapter_number)
- `TOCEntry` - TOC entry data
- `AlignmentIssue` - Issue found during validation
- `ValidationReport` - Complete report with detailed metrics

**Issue Types**:
- `missing_toc` - No TOC found in front_matter
- `missing_chapters` - No chapters found in body
- `count_mismatch` - TOC count ‚â† chapter count
- `missing_chapter` - TOC references chapter that doesn't exist (e.g., Á¨¨‰∏ÄÁ´† in TOC but body starts at Á¨¨‰∫åÁ´†)
- `duplicate_chapter_number` - Same chapter number appears multiple times
- `title_mismatch` - TOC title doesn't match actual heading
- `chapter_not_in_toc` - Chapter exists but not in TOC
- `sequence_gap` - Missing chapter numbers in sequence

**Usage**:
```python
from utils.toc_chapter_validator import TOCChapterValidator

validator = TOCChapterValidator(use_ai=True)
report = validator.validate(cleaned_json)

# Report includes:
# - toc_entries: List of TOC entries
# - chapter_headings: List of extracted headings with actual chapter numbers
# - issues: Detailed list of problems
# - confidence_score: Match percentage
# - is_valid: True if no errors

# Save detailed report
report = validator.validate_file('cleaned_book.json', save_report=True)
# Generates: cleaned_book_validation_report.json
```

**Report Metrics**:
- `toc_count` - Number of TOC entries
- `chapter_count` - Number of body chapters
- `matched_count` - Successfully matched pairs
- `confidence_score` - Match percentage (0-100%)
- `is_valid` - True if no errors
- `summary` - Human-readable summary
- `issues` - List with severity, type, message, details, suggested_fix

**AI Validation**:
- Only used for ambiguous title mismatches
- Batch processing (10 pairs per call)
- Model: gpt-4o-mini, temperature: 0.1
- Classifies mismatches as: real_mismatch, minor_difference, transcription_error
- Provides suggested fixes for typos

**Example Output**:
```
TOC/CHAPTER ALIGNMENT VALIDATION REPORT
Summary: TOC Entries: 20 | Body Chapters: 19 | Matched: 19 | Confidence: 95.0% | Errors: 1
Valid: ‚úó No

TOC ENTRIES (20):
   1. Á¨¨‰∏ÄÁ´†„ÄÄÁ•ûÁßòÁöÑÂπ¥Ëºï‰∫∫
   2. Á¨¨‰∫åÁ´†„ÄÄÂæûÂ§©ËÄåÈôçÁöÑÊïëÊòü
   ...

BODY CHAPTERS (19):
   2. Á¨¨‰∫åÁ´†„ÄÄÂæûÂ§©ËÄåÈôçÁöÑÊïëÊòü [body_chapter]
   3. Á¨¨‰∏âÁ´†„ÄÄÂ§ßËÆäÂøΩÁÑ∂‰æÜ [body_chapter]
   ...

ISSUES FOUND (2):
‚úó [ERROR] missing_chapter
   TOC references chapter 1 'Á•ûÁßòÁöÑÂπ¥Ëºï‰∫∫' but it's not in body
   üí° Suggested fix: Check if chapter is missing from source EPUB or was incorrectly filtered
```

**Integration**:
- Used in `batch_process_books.py` as part of Stage 6 validation
- CLI available: `scripts/validate_toc_chapter_alignment.py`
- Generates detailed JSON reports for debugging

### Content Structurer (processors/content_structurer.py)

**Classes**:
- `ProcessingConfig` - Configuration dataclass (max_retries, timeout, mode)
- `SchemaValidator` - Validates against JSON schema
- `TextChunker` - Splits large texts (max 4000 chars, 200 overlap)
- `ContentStructuringProcessor` - Main processor with retry/batch support

**Processing Modes**:
- STRICT - Fail on first error
- FLEXIBLE (default) - Retry most errors
- BEST_EFFORT - Continue despite errors

**Semantic Block Types**:
- `narrative`, `dialogue`, `verse`, `document`, `thought`, `descriptive`, `chapter_title`

**OpenAI Assistant Pattern**:
```python
thread = client.beta.threads.create()
client.beta.threads.messages.create(thread_id, role="user", content=text)
run = client.beta.threads.runs.create(thread_id, assistant_id)
# Poll until completed (max 300s)
messages = client.beta.threads.messages.list(thread_id)
# Parse JSON response
```

**Batch Processing**:
- ThreadPoolExecutor with configurable workers (default: 3)
- Rate limiting: 0.5s delay between requests
- Progress tracking with tqdm (optional)

### Structure Validator (processors/structure_validator.py)

**Purpose**: AI-powered validation of TOC/chapter alignment and structural classification

**Validation Checks**:
1. **TOC Coverage** - Ensures all chapters are represented in TOC
2. **TOC/Chapter Alignment** - Verifies TOC entries match actual chapter titles
3. **Chapter Numbering** - Checks for gaps and duplicates in ordinals
4. **Section Classification** - AI classifies chapters as front_matter, body, or back_matter
5. **Special Section Detection** - Identifies prefaces, afterwords, appendices, etc.

**Classes**:
- `StructureValidator` - Main validation engine
- `ValidationIssue` - Represents a single validation issue (error/warning/info)
- `ChapterClassification` - AI classification result for a chapter
- `ValidationResult` - Complete validation report with scores

**Section Types**:
- `FRONT_MATTER` - Preface, introduction, author notes
- `BODY` - Main story chapters
- `BACK_MATTER` - Afterword, appendix, notes

**Special Section Types** (Chinese novel structure):
- `preface` - Ëá™Â∫è, ÂâçË®Ä, Â∫è
- `introduction` - ÂºïË®Ä, Â∫èÁ´†
- `prologue` - Â∫èÂπï, Ê•îÂ≠ê
- `afterword` - ÂæåË®ò, Ë∑ã
- `appendix` - ÈôÑÈåÑ
- `author_note` - ‰ΩúËÄÖË®ª, Ë™™Êòé
- `epilogue` - Â∞æËÅ≤
- `main_chapter` - Regular story chapter

**AI Classification Pattern**:
```python
# Uses OpenAI to semantically analyze chapter titles
validator = StructureValidator(model="gpt-4o-mini", temperature=0.3)
result = validator.validate(cleaned_json_data)

# Graceful degradation: Falls back to basic validation if AI fails
# (API key issues, rate limits, etc.)
```

**Output Metrics**:
- `toc_coverage` - Percentage of chapters in TOC (0-100%)
- `structure_quality` - Overall quality score (0-100)
- `is_valid` - Boolean indicating no critical errors
- `issues` - List of errors/warnings with suggestions

**Integration with Batch Pipeline**:
- Automatically runs as Stage 5 in `batch_process_books.py`
- Falls back to basic validation if OpenAI API unavailable
- Generates detailed validation reports (JSON)

**Usage Example**:
```python
from processors.structure_validator import StructureValidator

validator = StructureValidator()
result = validator.process_file(
    input_path="cleaned_book.json",
    save_report=True  # Saves to {input}_validation.json
)

print(f"Valid: {result.is_valid}")
print(f"TOC Coverage: {result.toc_coverage}%")
print(f"Quality Score: {result.structure_quality}/100")

for issue in result.issues:
    print(f"[{issue.severity}] {issue.message}")
```

**Common Issues Detected**:
- Partial title mismatches (e.g., TOC has decorators like "‚òÜ‚òÜ‚òÜ" not in chapter)
- Missing chapters from TOC
- Invalid TOC references (pointing to non-existent chapters)
- Out-of-order sections (e.g., afterword before main chapters)
- Duplicate or missing chapter ordinals

### Assistant Manager (ai/assistant_manager.py)

**Purpose**: Centralized OpenAI assistant lifecycle management with versioning

**Storage**: `.assistants/` directory (JSON files)

**Key Methods**:
- `create_assistant(name, instructions, schema, model, temperature)`
- `get_assistant(name, version)` - version can be "v1", "v2", "latest"
- `list_assistants()` - All stored configs
- `update_assistant(name, updates)` - Modify existing
- `delete_assistant(name, version, remote=False)` - Remove with optional API deletion
- `export_assistant(name, version)` - Export config
- `import_assistant(file_path)` - Import config
- `compare_versions(name, v1, v2)` - Show differences

**Versioning**: Supports semantic versions, tracks changes, "latest" keyword

## Configuration & Environment

### Required Environment Variables
```bash
export OPENAI_API_KEY=your-key-here        # For AI features
export ANTHROPIC_API_KEY=your-key-here     # Optional alternative
```

### Default Paths
- Input: `./input/book.json`
- Output: `./output/cleaned_book.json`
- Assistants: `.assistants/` (AI configs)
- Schemas: `schemas/` (JSON schemas)

### Processing Defaults

**json_cleaner.py**:
- DEFAULT_INPUT_PATH: `./input/book.json`
- DEFAULT_OUTPUT_PATH: `./output/cleaned_book.json`
- DEFAULT_LANGUAGE: `zh-Hant`

**content_structurer.py**:
- max_retries: 3
- retry_delay: 2.0s
- rate_limit_delay: 0.5s
- timeout: 300s (5 min)
- max_workers: 3
- max_chunk_size: 4000 chars
- chunk_overlap: 200 chars

**structure_validator.py**:
- model: "gpt-4o-mini"
- temperature: 0.3 (low for consistent validation)
- timeout: 60s (1 min)
- save_report: True (generates validation JSON)

**toc_alignment_validator.py**:
- model: "gpt-4o-mini"
- temperature: 0.1 (very low for consistency)
- batch_size: 20 (TOC/chapter pairs per API call)

**batch_process_books.py**:
- catalog_path: Required (path to SQLite catalog database)
- dry_run: False (set True to skip file writes)
- limit: None (process all files, or set number for testing)
- 6-stage pipeline: topology ‚Üí sanity_check ‚Üí cleaning ‚Üí alignment ‚Üí toc ‚Üí validation

## Dependencies

From `requirements.txt`:
- `openai>=1.0.0` - OpenAI API client
- `anthropic>=0.18.0` - Anthropic API client
- `httpx>=0.24.0` - Async HTTP
- `beautifulsoup4>=4.12.0` + `lxml>=4.9.0` - HTML parsing
- `tenacity>=8.2.0` - Retry logic
- `tqdm>=4.65.0` - Progress bars
- `pytest>=7.4.0` - Testing

## Common Workflows

### Add New Block Type

1. Update `SchemaValidator.validate()` in content_structurer.py:
   ```python
   valid_types = [..., "new_type"]
   ```
2. Update assistant instructions to recognize new type
3. Update schema file if using validation

### Process Large Book

```bash
# Automatic chunking for >4000 chars
python cli/structure.py --input large_novel.txt --output result.json

# Disable chunking (may hit token limits)
python cli/structure.py --input novel.txt --no-chunking
```

### Debug AI Assistant

```bash
# List all assistants
python ai/assistant_manager.py list

# View specific config
python ai/assistant_manager.py export --name structuring --version latest

# Compare versions
python ai/assistant_manager.py compare --name structuring --version1 v1 --version2 v2
```

### Batch Processing (6-Stage Pipeline)

**Complete Pipeline** (recommended):
```bash
# Process all files with full pipeline
python scripts/batch_process_books.py \
  --source-dir /path/to/source_files \
  --output-dir /path/to/output \
  --catalog-path /path/to/wuxia_catalog.db \
  --log-dir ./logs

# Test on subset
python scripts/batch_process_books.py \
  --source-dir /path/to/source_files \
  --output-dir /path/to/output \
  --catalog-path /path/to/wuxia_catalog.db \
  --limit 10  # Process first 10 files only
```

**Individual Stage Processing**:
```bash
# Run specific post-processing stages
python utils/fix_chapter_alignment.py --input cleaned_book.json
python utils/restructure_toc.py --input cleaned_book.json
python -m utils.toc_alignment_validator cleaned_book.json

# Content structuring (separate from 6-stage pipeline)
python cli/structure.py --input ./books/ --output ./results/ --max-workers 5
```

**Pipeline Output**:
- Detailed JSON report in logs directory
- Stage-by-stage success rates
- File-level results with warnings and errors
- Performance metrics (time per file, tokens estimated)
- Issue categorization (topology errors, TOC mismatches, etc.)

### Implement New Processor

1. Create file in `processors/` (e.g., `translator.py`)
2. Implement processor class with `process()` method
3. Update `processors/__init__.py` to export it
4. Create CLI in `cli/` (e.g., `translate.py`)
5. Add entry point in `pyproject.toml` [project.scripts]
6. Add npm script in `package.json`
7. Update README.md roadmap

## File Organization

### Processors Module Structure
Each processor should follow this pattern:
```python
class ProcessorName:
    def __init__(self, **config):
        """Initialize with configuration"""

    def process(self, data: Dict) -> Dict:
        """Main processing method"""

    def process_file(self, input_path: str, output_path: str):
        """File-based processing"""

def main():
    """CLI entry point with argparse"""

if __name__ == "__main__":
    exit(main())
```

### CLI Module Structure
Each CLI should wrap a processor:
```python
#!/usr/bin/env python3
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from processors.module_name import main

if __name__ == "__main__":
    sys.exit(main())
```

## Testing Notes

Current test coverage is minimal (`tests/test_sanity.py` only).

When adding tests:
- Test each processor independently
- Mock AI API calls (don't hit real APIs in tests)
- Test edge cases: empty input, malformed JSON, large files
- Test chunking boundary conditions
- Test batch processing concurrency
- Validate output schema compliance

## Error Handling Patterns

### Retry Strategy (content_structurer.py)
- 3 attempts with 2s delay
- 300s timeout per request
- Mode-dependent: FLEXIBLE retries, STRICT fails fast
- Handles: JSON parse errors, validation failures, API failures

### Schema Validation
Checks:
- `content_blocks` array exists and non-empty
- Each block has: `id`, `type`, `content`, `metadata`
- Valid block types from predefined list
- IDs start with `block_`
- Non-empty content

### Batch Processing
- ThreadPoolExecutor catches individual failures
- Continues processing remaining files (unless STRICT mode)
- Returns: `{file: {status: "success|failed", result|error}}`

## Migration Notes

### v0.1.0 ‚Üí v0.2.0 (Current Refactoring)

**File Moves**:
- `clean_input_json.py` ‚Üí `processors/json_cleaner.py`
- `content_structuring_processor.py` ‚Üí `processors/content_structurer.py`
- `translation_assistant_manager.py` ‚Üí `ai/assistant_manager.py`
- `src/template_pkg/clients/*` ‚Üí `utils/clients/`
- `src/template_pkg/scraping/*` ‚Üí `utils/http/`
- `TRANSLATION_ASSISTANT_MANAGER_GUIDE.md` ‚Üí `docs/AI_ASSISTANT_GUIDE.md`

**CLI Changes**:
- Old: `python clean_input_json.py --input book.json`
- New: `python cli/clean.py --input book.json`

**Import Changes**:
- Old: `from clean_input_json import clean_book_json`
- New: `from processors.json_cleaner import clean_book_json`

## Known Issues and Limitations

### Chapter Alignment Fixer

**Issue**: Assumes books start at Chapter 1

**Problem**:
- The fixer in `utils/fix_chapter_alignment.py` assumes all books start at Chapter 1
- Some books (e.g., volume 3 of a series) start at Chapter 2 or higher
- This causes incorrect `chapter_number` fields in TOC entries

**Example**:
- Book: ÁæÖÂâéÂ§´‰∫∫ (I0929) - Actually starts at Á¨¨‰∫åÁ´† (Chapter 2)
- After processing: TOC shows Á¨¨‰∏ÄÁ´† pointing to title page instead of first actual chapter
- Result: TOC chapter_number fields don't match actual chapter headings

**Workaround**:
- Sanity checker detects this as `nonstandard_start` issue (info severity)
- Validation stage may flag TOC/chapter mismatches
- Consider fixing the alignment fixer to detect and respect actual starting chapter

### Chinese Numeral Parsing

**Fixed**: Special numerals Âªø (20), ÂçÖ (30), Âçå (40) now fully supported

**Historical Issue**:
- Before fix: Á¨¨Âªø‰∏ÄÁ´† was parsed as 1 instead of 21
- Caused false duplicate chapter errors
- Fixed in `chapter_sequence_validator.py` with special case handling

### TOC Blob Parsing

**Edge Case**: Space-separated entries on single line

**Handling**:
- `restructure_toc.py` uses careful pattern matching
- Avoids over-splitting on internal spaces in chapter titles
- Supports both blob format and structured format

## Roadmap Awareness

When implementing features, reference the roadmap in README.md:

- **v0.3.0**: Translator processor (language detection, glossaries)
- **v0.4.0**: Footnote generator (citation styles, cultural notes)
- **v0.5.0**: EPUB builder (EPUB 3.0, CSS themes, cover images)

Placeholders exist for all future processors with TODO comments indicating planned features.
- the source json files are individual diretoriestries  in a directory  /Users/jacki/project_files/translation_project/wuxia_individual_files